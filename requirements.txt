# Core dependencies
torch>=2.4.0
triton>=3.0.0
transformers>=4.51.0
xxhash
rich>=14.1.0
numpy>=1.24.0
tqdm>=4.65.0

# GPU power monitoring (optional, Linux only)
nvidia-ml-py3>=11.0.0; platform_system == 'Linux'

# Flash Attention (OPTIONAL - only for Ampere+ GPUs like A100/H100/3090/4090)
# V100 and older GPUs do NOT support flash-attn
# Uncomment the line below if you have compatible GPU:
# flash-attn

